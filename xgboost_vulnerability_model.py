import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold
import pickle
import os
import warnings
import time
import psutil
import gc
warnings.filterwarnings('ignore')

class XGBoostVulnerabilityDetector:
    def __init__(self, data_path="E:/project dataset/processed_data/processed_data"):
        """
        Initialize XGBoost Vulnerability Detector
        """
        self.data_path = data_path
        self.model = None
        self.best_params = None
        self.results = {}
        
        # Hardware info
        self.memory_limit = psutil.virtual_memory().available / (1024**3)
        print(f"Available RAM: {self.memory_limit:.1f}GB")
        
        # Batch size optimization
        self.batch_size = min(10000, max(1000, int(self.memory_limit * 500)))
        print(f"Optimized batch size: {self.batch_size}")
    
    def load_data_in_batches(self):
        """Load data efficiently"""
        print("Loading data...")
        start_time = time.time()
        
        try:
            # Load CSV files
            print("- Loading training data...")
            self.train_df = pd.read_csv(os.path.join(self.data_path, 'train_features.csv'))
            
            print("- Loading validation data...")
            self.val_df = pd.read_csv(os.path.join(self.data_path, 'val_features.csv'))
            
            print("- Loading test data...")
            self.test_df = pd.read_csv(os.path.join(self.data_path, 'test_features.csv'))
            
            load_time = time.time() - start_time
            print(f"Data loaded successfully in {load_time:.2f} seconds")
            
            # Data info
            print(f"\nDataset Information:")
            print(f"  Train samples: {len(self.train_df):,}")
            print(f"  Validation samples: {len(self.val_df):,}")
            print(f"  Test samples: {len(self.test_df):,}")
            print(f"  Features: {len(self.train_df.columns) - 1}")
            
            # Class distribution
            train_dist = self.train_df['vul'].value_counts()
            print(f"\nTarget Distribution:")
            print(f"  Non-vulnerable: {train_dist[0]:,} ({train_dist[0]/len(self.train_df)*100:.1f}%)")
            print(f"  Vulnerable: {train_dist[1]:,} ({train_dist[1]/len(self.train_df)*100:.1f}%)")
            
            return True
            
        except Exception as e:
            print(f"Error loading data: {e}")
            return False
    
    def prepare_features(self):
        """Prepare features for XGBoost training"""
        print("\nPreparing features for XGBoost...")
        
        # Separate features and targets
        feature_cols = [col for col in self.train_df.columns if col != 'vul']
        
        self.X_train = self.train_df[feature_cols].values
        self.y_train = self.train_df['vul'].values
        
        self.X_val = self.val_df[feature_cols].values
        self.y_val = self.val_df['vul'].values
        
        self.X_test = self.test_df[feature_cols].values
        self.y_test = self.test_df['vul'].values
        
        # Handle any missing values
        self.X_train = np.nan_to_num(self.X_train, 0)
        self.X_val = np.nan_to_num(self.X_val, 0)
        self.X_test = np.nan_to_num(self.X_test, 0)
        
        print(f"Features prepared:")
        print(f"  Feature matrix shape: {self.X_train.shape}")
        
        # Feature names for importance analysis
        self.feature_names = feature_cols
        print(f"  Total features: {len(self.feature_names)}")
        print(f"  Feature names: {self.feature_names}")
    
    def hyperparameter_tuning(self):
        """Perform hyperparameter tuning"""
        print("\nStarting hyperparameter tuning...")
        start_time = time.time()
        
        # Base parameters
        base_params = {
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'random_state': 42,
            'verbosity': 0,
            'n_jobs': 4,
            'tree_method': 'hist',
            'max_bin': 512,
        }
        
        # Parameter space
        param_dist = {
            'max_depth': [3, 4, 5, 6],
            'learning_rate': [0.05, 0.1, 0.15, 0.2],
            'n_estimators': [100, 200, 300],
            'subsample': [0.8, 0.9, 1.0],
            'colsample_bytree': [0.8, 0.9, 1.0],
            'min_child_weight': [1, 3, 5],
            'gamma': [0, 0.1, 0.2],
            'reg_alpha': [0, 0.1, 0.5],
            'reg_lambda': [1, 1.5, 2],
        }
        
        # Create XGBoost classifier
        xgb_classifier = xgb.XGBClassifier(**base_params)
        
        # Stratified K-Fold for imbalanced data
        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
        
        # Randomized search
        random_search = RandomizedSearchCV(
            estimator=xgb_classifier,
            param_distributions=param_dist,
            n_iter=15,
            cv=cv,
            scoring='roc_auc',
            n_jobs=1,
            verbose=1,
            random_state=42
        )
        
        print(f"Testing hyperparameters with 3-fold CV...")
        
        # Perform search
        random_search.fit(self.X_train, self.y_train)
        
        tuning_time = time.time() - start_time
        print(f"Hyperparameter tuning completed in {tuning_time/60:.1f} minutes")
        
        self.best_params = random_search.best_params_
        print(f"\nBest parameters found:")
        for param, value in self.best_params.items():
            print(f"  {param}: {value}")
        print(f"Best CV Score (AUC): {random_search.best_score_:.4f}")
        
        return random_search.best_estimator_
    
    def train_xgboost_simple(self, use_tuned_params=True):
        """Train XGBoost with simple configuration"""
        print("\nStarting XGBoost training...")
        start_time = time.time()
        
        # Base parameters
        base_params = {
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'random_state': 42,
            'verbosity': 1,
            'n_jobs': 4,
            'tree_method': 'hist',
            'max_bin': 512,
        }
        
        if use_tuned_params and self.best_params:
            # Use tuned parameters
            base_params.update(self.best_params)
            print("Using tuned hyperparameters")
        else:
            # Use default parameters
            default_params = {
                'max_depth': 5,
                'learning_rate': 0.1,
                'n_estimators': 200,
                'subsample': 0.9,
                'colsample_bytree': 0.9,
                'min_child_weight': 3,
                'gamma': 0.1,
                'reg_alpha': 0.1,
                'reg_lambda': 1.5,
            }
            base_params.update(default_params)
            print("Using default parameters")
        
        # Calculate class weights
        pos_weight = len(self.y_train[self.y_train == 0]) / len(self.y_train[self.y_train == 1])
        base_params['scale_pos_weight'] = pos_weight
        print(f"Class weight (scale_pos_weight): {pos_weight:.2f}")
        
        # Create and train model
        self.model = xgb.XGBClassifier(**base_params)
        
        n_estimators = base_params.get('n_estimators', 200)
        print(f"Training with {n_estimators} estimators...")
        
        # Simple training without early stopping
        self.model.fit(self.X_train, self.y_train)
        
        training_time = time.time() - start_time
        print(f"Training completed in {training_time/60:.1f} minutes")
        
        return self.model
    
    def evaluate_model(self):
        """Evaluate model performance"""
        print("\nEvaluating XGBoost model...")
        
        # Generate predictions
        print("Generating predictions...")
        
        train_pred_proba = self.model.predict_proba(self.X_train)[:, 1]
        train_pred = self.model.predict(self.X_train)
        
        val_pred_proba = self.model.predict_proba(self.X_val)[:, 1]
        val_pred = self.model.predict(self.X_val)
        
        test_pred_proba = self.model.predict_proba(self.X_test)[:, 1]
        test_pred = self.model.predict(self.X_test)
        
        # Calculate metrics
        datasets = {
            'Train': (self.y_train, train_pred, train_pred_proba),
            'Validation': (self.y_val, val_pred, val_pred_proba),
            'Test': (self.y_test, test_pred, test_pred_proba)
        }
        
        self.results = {}
        
        for split_name, (y_true, y_pred, y_proba) in datasets.items():
            metrics = {
                'accuracy': accuracy_score(y_true, y_pred),
                'precision': precision_score(y_true, y_pred, zero_division=0),
                'recall': recall_score(y_true, y_pred, zero_division=0),
                'f1': f1_score(y_true, y_pred, zero_division=0),
                'auc_roc': roc_auc_score(y_true, y_proba)
            }
            
            self.results[split_name] = metrics
            
            print(f"\n{split_name} Set Performance:")
            for metric_name, value in metrics.items():
                print(f"  {metric_name.capitalize()}: {value:.4f}")
        
        # Classification report for test set
        print(f"\nTest Set Classification Report:")
        print(classification_report(self.y_test, test_pred))
        
        return self.results
    
    def analyze_feature_importance(self):
        """Analyze feature importance"""
        print(f"\nAnalyzing feature importance...")
        
        # Get feature importance
        importance = self.model.feature_importances_
        feature_imp = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        print(f"\nFeature Importance Ranking:")
        for i, (_, row) in enumerate(feature_imp.iterrows(), 1):
            print(f"{i:2d}. {row['feature']}: {row['importance']:.4f}")
        
        # Plot feature importance
        plt.figure(figsize=(10, 6))
        plt.barh(range(len(feature_imp)), feature_imp['importance'][::-1])
        plt.yticks(range(len(feature_imp)), feature_imp['feature'][::-1])
        plt.xlabel('Feature Importance')
        plt.title('XGBoost Feature Importance')
        plt.tight_layout()
        
        # Create results directory
        os.makedirs('./results', exist_ok=True)
        plt.savefig('./results/xgboost_feature_importance.png', dpi=150, bbox_inches='tight')
        plt.show()
        
        # Save feature importance
        feature_imp.to_csv('./results/xgboost_feature_importance.csv', index=False)
        print(f"Feature importance saved")
        
        return feature_imp
    
    def plot_results(self):
        """Plot training results"""
        print("\nGenerating result plots...")
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        # Performance comparison
        metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']
        train_scores = [self.results['Train'][m] for m in metrics]
        val_scores = [self.results['Validation'][m] for m in metrics]
        test_scores = [self.results['Test'][m] for m in metrics]
        
        x_pos = np.arange(len(metrics))
        width = 0.25
        
        axes[0, 0].bar(x_pos - width, train_scores, width, label='Train', alpha=0.8)
        axes[0, 0].bar(x_pos, val_scores, width, label='Validation', alpha=0.8)
        axes[0, 0].bar(x_pos + width, test_scores, width, label='Test', alpha=0.8)
        axes[0, 0].set_title('Performance Metrics Comparison')
        axes[0, 0].set_xlabel('Metrics')
        axes[0, 0].set_ylabel('Score')
        axes[0, 0].set_xticks(x_pos)
        axes[0, 0].set_xticklabels(metrics, rotation=45)
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Confusion Matrix
        test_pred = self.model.predict(self.X_test)
        cm = confusion_matrix(self.y_test, test_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])
        axes[0, 1].set_title('Test Set Confusion Matrix')
        axes[0, 1].set_xlabel('Predicted')
        axes[0, 1].set_ylabel('Actual')
        
        # ROC Curve
        test_pred_proba = self.model.predict_proba(self.X_test)[:, 1]
        fpr, tpr, _ = roc_curve(self.y_test, test_pred_proba)
        auc_score = roc_auc_score(self.y_test, test_pred_proba)
        
        axes[1, 0].plot(fpr, tpr, label=f'XGBoost (AUC = {auc_score:.4f})', linewidth=2)
        axes[1, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5)
        axes[1, 0].set_xlabel('False Positive Rate')
        axes[1, 0].set_ylabel('True Positive Rate')
        axes[1, 0].set_title('ROC Curve')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # Feature importance
        importance = self.model.feature_importances_
        feature_imp = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        axes[1, 1].barh(range(len(feature_imp)), feature_imp['importance'][::-1])
        axes[1, 1].set_yticks(range(len(feature_imp)))
        axes[1, 1].set_yticklabels(feature_imp['feature'][::-1])
        axes[1, 1].set_xlabel('Feature Importance')
        axes[1, 1].set_title('Feature Importance')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('./results/xgboost_results.png', dpi=150, bbox_inches='tight')
        plt.show()
        
        print("Result plots generated")
    
    def save_model(self):
        """Save model and results"""
        print("\nSaving model and results...")
        
        # Create directories
        os.makedirs('./models', exist_ok=True)
        os.makedirs('./results', exist_ok=True)
        
        # Save model
        with open('./models/xgboost_model.pkl', 'wb') as f:
            pickle.dump(self.model, f)
        
        # Save results
        results_df = pd.DataFrame(self.results).T
        results_df.to_csv('./results/xgboost_results.csv')
        
        # Save model info
        model_info = {
            'model_type': 'XGBoost',
            'training_samples': len(self.X_train),
            'features': len(self.feature_names),
            'feature_names': self.feature_names,
            'best_params': str(self.best_params) if self.best_params else 'Default',
            'test_auc': self.results['Test']['auc_roc'],
            'test_f1': self.results['Test']['f1'],
            'test_accuracy': self.results['Test']['accuracy']
        }
        
        with open('./models/xgboost_info.txt', 'w') as f:
            for key, value in model_info.items():
                f.write(f"{key}: {value}\n")
        
        print("Model saved successfully")
        print("Files saved:")
        print("- ./models/xgboost_model.pkl")
        print("- ./results/xgboost_results.csv")
        print("- ./models/xgboost_info.txt")
    
    def run_pipeline(self, tune_hyperparameters=True):
        """Run complete pipeline"""
        total_start = time.time()
        
        print("="*60)
        print("XGBOOST VULNERABILITY DETECTION PIPELINE")
        print("="*60)
        
        # Load data
        if not self.load_data_in_batches():
            print("Failed to load data")
            return False
        
        # Prepare features
        self.prepare_features()
        
        # Hyperparameter tuning
        if tune_hyperparameters:
            print("\n" + "="*40)
            print("HYPERPARAMETER TUNING")
            print("="*40)
            try:
                self.hyperparameter_tuning()
                gc.collect()
            except Exception as e:
                print(f"Hyperparameter tuning failed: {e}")
                print("Using default parameters...")
                tune_hyperparameters = False
        
        # Train model
        print("\n" + "="*40)
        print("TRAINING MODEL")
        print("="*40)
        try:
            self.train_xgboost_simple(use_tuned_params=tune_hyperparameters)
        except Exception as e:
            print(f"Training failed: {e}")
            return False
        
        # Evaluate model
        print("\n" + "="*40)
        print("MODEL EVALUATION")
        print("="*40)
        self.evaluate_model()
        
        # Feature importance
        print("\n" + "="*40)
        print("FEATURE IMPORTANCE")
        print("="*40)
        self.analyze_feature_importance()
        
        # Plot results
        print("\n" + "="*40)
        print("GENERATING PLOTS")
        print("="*40)
        self.plot_results()
        
        # Save model
        self.save_model()
        
        total_time = time.time() - total_start
        print(f"\n" + "="*60)
        print(f"PIPELINE COMPLETED IN {total_time/60:.1f} MINUTES")
        print(f"Test AUC: {self.results['Test']['auc_roc']:.4f}")
        print(f"Test F1: {self.results['Test']['f1']:.4f}")
        print(f"Test Accuracy: {self.results['Test']['accuracy']:.4f}")
        print("="*60)
        
        return True

if __name__ == "__main__":
    print("Starting XGBoost Pipeline")
    print(f"Time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Initialize detector
    detector = XGBoostVulnerabilityDetector(
        data_path="E:/project dataset/processed_data/processed_data"
    )
    
    # Run pipeline
    success = detector.run_pipeline(tune_hyperparameters=True)
    
    if success:
        print("\nPipeline completed successfully!")
        print("Check ./results/ for plots")
        print("Check ./models/ for saved model")
    else:
        print("Pipeline failed")
    
    print(f"Finished: {time.strftime('%Y-%m-%d %H:%M:%S')}")